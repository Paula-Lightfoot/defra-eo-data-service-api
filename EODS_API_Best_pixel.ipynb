{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for this Notebook to work correctly, the virtual memomry Windows 10 machine the Notebook is beign run must be increased. This is beacuse the notebook will download and process sattelite images that genneraly very large. In order to achive this, ther are some system modifications that need to be done before running the Notebook. To increase the virtual memory of the machine: Go to Control panel---> System---> Advanced System Settings. In the window that appears select the Advance tab then settings, in the new window select the advanced tab , the clicl on change. Here untick the \"Automatically manage paging file for all drives\" Select the Custom size and in the intial size (MB) box enter 4096 and in the Maximumsize (MB) box enter 7000 or 8000. (The maximum size can be increased if the mosai to be created is very large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the neccesary libraries for downloading data \n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xmltodict\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will create a folder, where the notebook is located, and all\n",
    "the downloaded images will be stored there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test1\n"
     ]
    }
   ],
   "source": [
    "# get input for \"test name\" eg 'PRD_STANDARDISED_TEST_WPS_S1x10_RUN1'\n",
    "RUNNAME = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fmt='%Y%m%dT%H%M%S'\n",
    "pretty_fmt='%Y-%m-%d %H:%M:%S'\n",
    "headers = {'Content-type': 'application/xml','User-Agent': 'curl/7.65.1'}\n",
    "\n",
    "wps_test_config = {\n",
    "        'ras:CropCoverage':{\n",
    "            'template_xml':'rasterclip-template.xml',\n",
    "            'mimetype':'image/tiff',\n",
    "            'output':'result.tiff',\n",
    "        },\n",
    "        'gs:Download':{\n",
    "            'template_xml':'gsdownload-template.xml',\n",
    "            'mimetype':'application/zip',\n",
    "            'output':'result.zip',\n",
    "        },\n",
    "        'reproject':{\n",
    "            'template_xml':'reproject_template.xml',\n",
    "            'mimetype':'application/zip',\n",
    "            'output':'result.zip',\n",
    "        },\n",
    "        'bandselect':{\n",
    "            'template_xml':'reproject_template.xml',\n",
    "            'mimetype':'application/zip',\n",
    "            'output':'result.zip',\n",
    "        }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it must be noted that this script in order to work, the input images must be of the same granule from different time stamps. This script will not create a mosaic of diferent granules but a mosiac of the same granule without any cloud coverage using diferent sensing dates of the same granule.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of granules from dowloaded csv \n",
    "#(Those are the resulting csv from eods-api-example-generate-cloudless-mosaic Notebook)\n",
    "col_list = [\"typename\"]\n",
    "min_cloud_per_granule = pd.read_csv(r\"C:\\Users\\Kostas.Sideris\\data\\min_cloud_per_granule.csv\")\n",
    "\n",
    "min_cloud_per_granule_per_orbit = pd.read_csv(r\"C:\\Users\\Kostas.Sideris\\data\\min-cloud-per-granule-per-orbit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USER INPUT\n",
    "### Enter your token in PRD_DM = \n",
    "\n",
    "wps_job_request_list = [\n",
    "     {'tool':'gs:Download','layer_list':min_cloud_per_granule_per_orbit[\"typename\"],'dl_bool':True}\n",
    "]\n",
    "\n",
    "PRD_DM = \"3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl\"\n",
    "ACCESS_TOKEN = PRD_DM\n",
    "\n",
    "ENV_PRD = 'https://earthobs.defra.gov.uk'\n",
    "wps_env = ENV_PRD\n",
    "\n",
    "wps_server = wps_env + '/geoserver/ows/?access_token=' + ACCESS_TOKEN + '&SERVICE=WPS&VERSION=1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-09-10T09:46:06.760697 :: tool=gs:Download, lyr=geonode:S2A_20190224_lat52lon075_T30UXC_ORB037_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref, download=True\n",
      "\n",
      "2020-09-10T09:46:08.830066 :: request POST url =https://earthobs.defra.gov.uk/geoserver/ows/?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&SERVICE=WPS&VERSION=1.0.0&REQUEST=EXECUTE\n",
      "\n",
      "2020-09-10T09:46:08.830066:: WPS job 1fb15a98-06e8-4853-948e-e6816fdb21a8 was successfully submitted\n",
      "\n",
      "2020-09-10T09:46:08.830066:: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?><wps:ExecuteResponse xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xmlns:ows=\"http://www.opengis.net/ows/1.1\" xmlns:wps=\"http://www.opengis.net/wps/1.0.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" service=\"WPS\" serviceInstance=\"https://earthobs.defra.gov.uk/geoserver/ows?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&amp;\" statusLocation=\"https://earthobs.defra.gov.uk/geoserver/ows?service=WPS&amp;version=1.0.0&amp;request=GetExecutionStatus&amp;executionId=1fb15a98-06e8-4853-948e-e6816fdb21a8&amp;access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl\" version=\"1.0.0\"><wps:Process wps:processVersion=\"1.0.0\"><ows:Identifier>gs:Download</ows:Identifier><ows:Title>Enterprise Download Process</ows:Title><ows:Abstract>Downloads Layer Stream and provides a ZIP.</ows:Abstract></wps:Process><wps:Status creationTime=\"2020-09-10T09:46:05.459Z\"><wps:ProcessAccepted>Process accepted.</wps:ProcessAccepted></wps:Status></wps:ExecuteResponse>'\n",
      "\n",
      "2020-09-10T09:46:08.830066 :: tool=gs:Download, lyr=geonode:S2A_20190330_lat52lon075_T30UXC_ORB094_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref, download=True\n",
      "\n",
      "2020-09-10T09:46:09.905743 :: request POST url =https://earthobs.defra.gov.uk/geoserver/ows/?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&SERVICE=WPS&VERSION=1.0.0&REQUEST=EXECUTE\n",
      "\n",
      "2020-09-10T09:46:09.905743:: WPS job 974386f7-a7bf-4273-8f97-a8243e4584bf was successfully submitted\n",
      "\n",
      "2020-09-10T09:46:09.905743:: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?><wps:ExecuteResponse xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xmlns:ows=\"http://www.opengis.net/ows/1.1\" xmlns:wps=\"http://www.opengis.net/wps/1.0.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" service=\"WPS\" serviceInstance=\"https://earthobs.defra.gov.uk/geoserver/ows?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&amp;\" statusLocation=\"https://earthobs.defra.gov.uk/geoserver/ows?service=WPS&amp;version=1.0.0&amp;request=GetExecutionStatus&amp;executionId=974386f7-a7bf-4273-8f97-a8243e4584bf&amp;access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl\" version=\"1.0.0\"><wps:Process wps:processVersion=\"1.0.0\"><ows:Identifier>gs:Download</ows:Identifier><ows:Title>Enterprise Download Process</ows:Title><ows:Abstract>Downloads Layer Stream and provides a ZIP.</ows:Abstract></wps:Process><wps:Status creationTime=\"2020-09-10T09:46:06.535Z\"><wps:ProcessAccepted>Process accepted.</wps:ProcessAccepted></wps:Status></wps:ExecuteResponse>'\n",
      "\n",
      "2020-09-10T09:46:09.905743 :: tool=gs:Download, lyr=geonode:S2A_20190221_lat52lon075_T30UXC_ORB137_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref, download=True\n",
      "\n",
      "2020-09-10T09:46:11.135368 :: request POST url =https://earthobs.defra.gov.uk/geoserver/ows/?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&SERVICE=WPS&VERSION=1.0.0&REQUEST=EXECUTE\n",
      "\n",
      "2020-09-10T09:46:11.135368:: WPS job c8831197-c40a-4351-9e14-6b6d81f226c4 was successfully submitted\n",
      "\n",
      "2020-09-10T09:46:11.135368:: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?><wps:ExecuteResponse xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xmlns:ows=\"http://www.opengis.net/ows/1.1\" xmlns:wps=\"http://www.opengis.net/wps/1.0.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" service=\"WPS\" serviceInstance=\"https://earthobs.defra.gov.uk/geoserver/ows?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&amp;\" statusLocation=\"https://earthobs.defra.gov.uk/geoserver/ows?service=WPS&amp;version=1.0.0&amp;request=GetExecutionStatus&amp;executionId=c8831197-c40a-4351-9e14-6b6d81f226c4&amp;access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl\" version=\"1.0.0\"><wps:Process wps:processVersion=\"1.0.0\"><ows:Identifier>gs:Download</ows:Identifier><ows:Title>Enterprise Download Process</ows:Title><ows:Abstract>Downloads Layer Stream and provides a ZIP.</ows:Abstract></wps:Process><wps:Status creationTime=\"2020-09-10T09:46:07.741Z\"><wps:ProcessAccepted>Process accepted.</wps:ProcessAccepted></wps:Status></wps:ExecuteResponse>'\n",
      "\n",
      "2020-09-10T09:46:11.135368 :: tool=gs:Download, lyr=geonode:S2A_20190330_lat52lon07_T30UYC_ORB094_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref, download=True\n",
      "\n",
      "2020-09-10T09:46:12.409982 :: request POST url =https://earthobs.defra.gov.uk/geoserver/ows/?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&SERVICE=WPS&VERSION=1.0.0&REQUEST=EXECUTE\n",
      "\n",
      "2020-09-10T09:46:12.409982:: WPS job 7b9e73e1-afee-4446-9bb9-f1c493afb6e3 was successfully submitted\n",
      "\n",
      "2020-09-10T09:46:12.409982:: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?><wps:ExecuteResponse xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xmlns:ows=\"http://www.opengis.net/ows/1.1\" xmlns:wps=\"http://www.opengis.net/wps/1.0.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" service=\"WPS\" serviceInstance=\"https://earthobs.defra.gov.uk/geoserver/ows?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&amp;\" statusLocation=\"https://earthobs.defra.gov.uk/geoserver/ows?service=WPS&amp;version=1.0.0&amp;request=GetExecutionStatus&amp;executionId=7b9e73e1-afee-4446-9bb9-f1c493afb6e3&amp;access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl\" version=\"1.0.0\"><wps:Process wps:processVersion=\"1.0.0\"><ows:Identifier>gs:Download</ows:Identifier><ows:Title>Enterprise Download Process</ows:Title><ows:Abstract>Downloads Layer Stream and provides a ZIP.</ows:Abstract></wps:Process><wps:Status creationTime=\"2020-09-10T09:46:09.002Z\"><wps:ProcessAccepted>Process accepted.</wps:ProcessAccepted></wps:Status></wps:ExecuteResponse>'\n",
      "\n",
      "2020-09-10T09:46:12.409982 :: tool=gs:Download, lyr=geonode:S2A_20190422_lat52lon07_T30UYC_ORB137_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref, download=True\n",
      "\n",
      "2020-09-10T09:46:13.823556 :: request POST url =https://earthobs.defra.gov.uk/geoserver/ows/?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&SERVICE=WPS&VERSION=1.0.0&REQUEST=EXECUTE\n",
      "\n",
      "2020-09-10T09:46:13.823556:: WPS job bf74982d-ae35-4e82-9695-a9a3f0629665 was successfully submitted\n",
      "\n",
      "2020-09-10T09:46:13.823556:: b'<?xml version=\"1.0\" encoding=\"UTF-8\"?><wps:ExecuteResponse xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xmlns:ows=\"http://www.opengis.net/ows/1.1\" xmlns:wps=\"http://www.opengis.net/wps/1.0.0\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" service=\"WPS\" serviceInstance=\"https://earthobs.defra.gov.uk/geoserver/ows?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&amp;\" statusLocation=\"https://earthobs.defra.gov.uk/geoserver/ows?service=WPS&amp;version=1.0.0&amp;request=GetExecutionStatus&amp;executionId=bf74982d-ae35-4e82-9695-a9a3f0629665&amp;access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl\" version=\"1.0.0\"><wps:Process wps:processVersion=\"1.0.0\"><ows:Identifier>gs:Download</ows:Identifier><ows:Title>Enterprise Download Process</ows:Title><ows:Abstract>Downloads Layer Stream and provides a ZIP.</ows:Abstract></wps:Process><wps:Status creationTime=\"2020-09-10T09:46:10.457Z\"><wps:ProcessAccepted>Process accepted.</wps:ProcessAccepted></wps:Status></wps:ExecuteResponse>'\n",
      "\n",
      "2020-09-10T09:46:13.824560 :: A TOTAL OF 5 WPS REQUESTS SUBMITTED\n",
      "\n",
      "2020-09-10T09:46:13.824560 :: The ExecutionIDs are :: dict_keys(['1fb15a98-06e8-4853-948e-e6816fdb21a8', '974386f7-a7bf-4273-8f97-a8243e4584bf', 'c8831197-c40a-4351-9e14-6b6d81f226c4', '7b9e73e1-afee-4446-9bb9-f1c493afb6e3', 'bf74982d-ae35-4e82-9695-a9a3f0629665'])\n",
      "\n",
      "2020-09-10T09:46:13.824560 :: WPS Check Executions URL is https://earthobs.defra.gov.uk/geoserver/ows/?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&SERVICE=WPS&VERSION=1.0.0&REQUEST=GetExecutions\n"
     ]
    }
   ],
   "source": [
    "# submitting the WPS request(s)\n",
    "\n",
    "def mod_the_xml(xml_name,layer_name):\n",
    "    \"\"\"\n",
    "    function read xml payload template and modify the payload with the config\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(Path.cwd() / 'xml' / xml_name,'r') as template_xml:\n",
    "        file_data = template_xml.read()\n",
    "    modded_xml = file_data.replace('template_layer_name', layer_name)\n",
    "    \n",
    "    return modded_xml\n",
    "\n",
    "def submit_wps_request(payload):\n",
    "    \"\"\"\n",
    "    function submit the wps POST request\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        wps_submit_response = requests.post(wps_server + '&REQUEST=EXECUTE', data=payload, headers=headers, verify=True)\n",
    "        print('\\n' + datetime.utcnow().isoformat() + ' :: request POST url =' + wps_server + '&REQUEST=EXECUTE')\n",
    "    except:\n",
    "        print('wps POST was not successful')\n",
    "    \n",
    "    # receive status code\n",
    "    timestamp_submission = datetime.utcnow()\n",
    "    status_code = wps_submit_response.status_code\n",
    "        \n",
    "    if status_code == 200 and not wps_submit_response.text.find('ExceptionReport') > 0:\n",
    "        execution_id = wps_submit_response.text.split('executionId=')[1].split('&')[0]        \n",
    "        print('\\n' + datetime.utcnow().isoformat() + ':: WPS job ' + execution_id + ' was successfully submitted')\n",
    "        print('\\n' + datetime.utcnow().isoformat() + ':: ' + str(wps_submit_response.content))\n",
    "        return execution_id\n",
    "    else:\n",
    "        print(datetime.utcnow().isoformat() + ' :: wps request was not successful :: STATUS = ' + str(status_code))\n",
    "        print(datetime.utcnow().isoformat() + ' :: response = ' + str(wps_submit_response.content))\n",
    "        return None\n",
    "\n",
    "execution_id_dict={}\n",
    "\n",
    "for item in wps_job_request_list:\n",
    "        \n",
    "    for lyr in item['layer_list']:\n",
    "        print('\\n' + datetime.utcnow().isoformat() + ' :: tool=' + item['tool'] + ', lyr=' + lyr + ', download=' + str(item['dl_bool']))\n",
    "        modded_xml = mod_the_xml(wps_test_config[item['tool']]['template_xml'],lyr)\n",
    "        execution_id = submit_wps_request(modded_xml)\n",
    "        execution_id_dict.update({execution_id:lyr})\n",
    "        submission_time = datetime.utcnow()\n",
    "\n",
    "print('\\n' + datetime.utcnow().isoformat() + ' :: A TOTAL OF ' + str(len(execution_id_dict)) +  ' WPS REQUESTS SUBMITTED')\n",
    "\n",
    "print('\\n' + datetime.utcnow().isoformat() + ' :: The ExecutionIDs are :: ' + str(execution_id_dict.keys()))\n",
    "\n",
    "print('\\n' + datetime.utcnow().isoformat() + ' :: WPS Check Executions URL is ' + wps_server + '&REQUEST=GetExecutions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#001 of #n   :: 2020-09-10T09:46:13.823556 :: Logging file: C:\\Users\\Kostas.Sideris\\output\\20200910T094613-Test1\\20200910T094613-Test1-wps-log.csv\n",
      "\n",
      "#001 of #n   :: 2020-09-10T09:46:23.790538 :: Poll Time\n",
      "#001 of #n   :: 2020-09-10T09:46:25.036164 :: 1 xml pages parsed in 1.0 seconds\n",
      "#001 of #n   :: 2020-09-10T09:46:25.100145 :: Jobs are still \"STATUS=RUNNING\"...script will poll again in 60sec time ...\n",
      "\n",
      "#002 of #n   :: 2020-09-10T09:47:25.083400 :: Poll Time\n",
      "#002 of #n   :: 2020-09-10T09:47:26.230054 :: 1 xml pages parsed in 1.0 seconds\n",
      "#002 of #n   :: 2020-09-10T09:47:26.256047 :: Jobs are still \"STATUS=RUNNING\"...script will poll again in 60sec time ...\n",
      "\n",
      "#003 of #n   :: 2020-09-10T09:48:26.241923 :: Poll Time\n",
      "#003 of #n   :: 2020-09-10T09:48:27.428589 :: 1 xml pages parsed in 1.0 seconds\n",
      "#003 of #n   :: 2020-09-10T09:48:27.470580 :: Jobs are still \"STATUS=RUNNING\"...script will poll again in 60sec time ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kostas.Sideris\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:69: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#004 of #n   :: 2020-09-10T09:49:27.454088 :: Poll Time\n",
      "#004 of #n   :: 2020-09-10T09:49:28.545785 :: 1 xml pages parsed in 1.0 seconds\n",
      "#004 of #n   :: 2020-09-10T09:49:28.575781 :: Jobs are still \"STATUS=RUNNING\"...script will poll again in 60sec time ...\n",
      "\n",
      "#005 of #n   :: 2020-09-10T09:50:28.559898 :: Poll Time\n",
      "#005 of #n   :: 2020-09-10T09:50:29.742585 :: 1 xml pages parsed in 1.0 seconds\n",
      "\n",
      "#005 of #005 :: 2020-09-10T09:50:29.779577 :: Script has finished\n"
     ]
    }
   ],
   "source": [
    "# poll the status page and wait until all jobs are completed\n",
    "\n",
    "# create output directory and log file\n",
    "Path(Path.cwd() / 'output').mkdir(parents=True, exist_ok=True)\n",
    "my_path = Path.cwd() / 'output' / str(submission_time.strftime(output_fmt) + '-' + RUNNAME)\n",
    "my_path.mkdir(parents=True, exist_ok=True)\n",
    "log_file_name = my_path / str(submission_time.strftime(output_fmt) + '-' + RUNNAME + '-wps-log.csv')\n",
    "poll_frequency_sec = 60\n",
    "frames = []\n",
    "\n",
    "# poll api for 1 hour unless all jobs submitted are NOT STATUS=RUNNING\n",
    "for i in range(1,3600):\n",
    "    \n",
    "    check_time = datetime.utcnow()\n",
    "    if i == 1:\n",
    "        print('\\n#' + format(i, '03') + ' of #n   :: ' + submission_time.isoformat() + ' :: Logging file: ' + str(log_file_name))\n",
    "    \n",
    "    print('\\n#' + format(i, '03') + ' of #n   :: ' + check_time.isoformat() + ' :: Poll Time')\n",
    "    \n",
    "    start_index = 0\n",
    "    matching_of_dicts = []\n",
    "    for page_counter in range(0,1000,10):\n",
    "\n",
    "        # constrcut the paging URL and make the request\n",
    "        all_status_url = wps_server + '&REQUEST=GetExecutions&startIndex=' + str(page_counter)\n",
    "\n",
    "        r = requests.get(all_status_url,headers=headers)\n",
    "\n",
    "        # parse the xml to an ordered dict using 3rd party imported module xmltodict\n",
    "        d = xmltodict.parse(r.content)\n",
    "\n",
    "        # grab the dicts of 'wps:ExecuteResponse' and add to a list\n",
    "        jobs_list = [value for value in d['wps:GetExecutionsResponse']['wps:ExecuteResponse']]\n",
    "\n",
    "        # add this to a master list\n",
    "        matching_of_dicts.append(jobs_list)\n",
    "\n",
    "        # if there is no 'next' attribute, then you're on the last xml page so break out the loop\n",
    "        if not any('@next' == key for key in list(d['wps:GetExecutionsResponse'].keys())):\n",
    "            break\n",
    "\n",
    "    num_of_xml_pages = int(page_counter / 10) + 1\n",
    "    duration_to_parse_xml_dt = datetime.utcnow() - check_time\n",
    "    duration_to_parse_xml_sec = round(duration_to_parse_xml_dt.total_seconds(),0)\n",
    "    print('#' + format(i, '03') + ' of #n   :: ' + datetime.utcnow().isoformat() + ' :: ' + str(num_of_xml_pages) + ' xml pages parsed in ' + str(duration_to_parse_xml_sec) + ' seconds')\n",
    "            \n",
    "    # now flatten the list into all responses\n",
    "    response_list = [item['wps:Status'] for sublist in matching_of_dicts for item in sublist]\n",
    "\n",
    "    # parse dict to pandas dataframe and set the index\n",
    "    df_temp = pd.DataFrame.from_dict(response_list)\n",
    "    df = df_temp.set_index('wps:JobID')\n",
    "\n",
    "    # filter df on current job_ids\n",
    "    filter_df  = df[df.index.isin(execution_id_dict.keys())]\n",
    "            \n",
    "    # add \"check time\" as column\n",
    "    filter_df.loc[:,'check_time'] = check_time.isoformat()\n",
    "    \n",
    "    # append on some extra info to the dataframe using the index\n",
    "    for index, row in filter_df.iterrows():\n",
    "        filter_df.loc[index,'layer_name'] = execution_id_dict[index]\n",
    "        filter_df.loc[index,'dl_url'] = wps_server + '&REQUEST=GetExecutionResult&EXECUTIONID='  + index + '&outputId=' + wps_test_config['gs:Download']['output'] + '&mimetype=' + wps_test_config['gs:Download']['mimetype']\n",
    "    \n",
    "    # sort out the indices and concatenate dataframes together and output to csv\n",
    "    filter_df.reset_index(inplace=True)\n",
    "    filter_df.set_index(['wps:JobID','check_time'],inplace=True)\n",
    "    frames.append(filter_df)\n",
    "    rolling_merged_df = pd.concat(frames)\n",
    "    rolling_merged_df.to_csv(log_file_name)\n",
    "        \n",
    "    # if NO running processes, then break, otherwise loop again\n",
    "    if not any(filter_df['wps:Status'] == 'RUNNING'):\n",
    "        break\n",
    "    print('#' + format(i, '03') + ' of #n   :: ' + datetime.utcnow().isoformat() + ' :: Jobs are still \"STATUS=RUNNING\"...script will poll again in ' + str(poll_frequency_sec) + 'sec time ...')\n",
    "    \n",
    "    time.sleep(poll_frequency_sec)\n",
    "    \n",
    "print('\\n#' + format(i, '03') + ' of #' + format(i, '03') + ' :: ' + datetime.utcnow().isoformat() + ' :: Script has finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export summary wps jobs with \"runtimes\"\n",
    "sumdf = rolling_merged_df.reset_index()\n",
    "grouped = sumdf.groupby(['wps:JobID','wps:Status']).first()\n",
    "grouped.to_csv(my_path / str(submission_time.strftime(output_fmt) + '-' + RUNNAME + '-summary-wps-log.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2020-09-10T09:50:56.310741 :: DOWNLOAD START :: URL = https://earthobs.defra.gov.uk/geoserver/ows/?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&SERVICE=WPS&VERSION=1.0.0&REQUEST=GetExecutionResult&EXECUTIONID=974386f7-a7bf-4273-8f97-a8243e4584bf&outputId=result.zip&mimetype=application/zip\n",
      "2020-09-10T09:51:12.316674 :: DOWNLOAD COMPLETE\n",
      "2020-09-10T09:51:12.316674 :: FILE WRITE START :: FILE = C:\\Users\\Kostas.Sideris\\output\\20200910T094613-Test1\\S2A_20190330_lat52lon075_T30UXC_ORB094_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.zip\n",
      "2020-09-10T09:51:32.154774 :: FILE WRITE COMPLETE\n",
      "2020-09-10T09:51:32.155772 :: EXTRACT ARCHIVE FILE STARTED\n",
      "2020-09-10T09:52:40.146192 :: EXTRACT ARCHIVE FILE COMPLETE\n",
      "2020-09-10T09:52:40.147188 :: RENAME/CLEAN UP DIRECTORY STARTED\n",
      "2020-09-10T09:52:40.450118 :: RENAME/CLEAN UP DIRECTORY COMPLETE\n",
      "\n",
      "\n",
      "2020-09-10T09:52:40.452118 :: DOWNLOAD START :: URL = https://earthobs.defra.gov.uk/geoserver/ows/?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&SERVICE=WPS&VERSION=1.0.0&REQUEST=GetExecutionResult&EXECUTIONID=7b9e73e1-afee-4446-9bb9-f1c493afb6e3&outputId=result.zip&mimetype=application/zip\n",
      "2020-09-10T09:54:13.767136 :: DOWNLOAD COMPLETE\n",
      "2020-09-10T09:54:13.768138 :: FILE WRITE START :: FILE = C:\\Users\\Kostas.Sideris\\output\\20200910T094613-Test1\\S2A_20190330_lat52lon07_T30UYC_ORB094_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.zip\n",
      "2020-09-10T09:55:15.146699 :: FILE WRITE COMPLETE\n",
      "2020-09-10T09:55:15.146699 :: EXTRACT ARCHIVE FILE STARTED\n",
      "2020-09-10T09:56:13.099126 :: EXTRACT ARCHIVE FILE COMPLETE\n",
      "2020-09-10T09:56:13.100127 :: RENAME/CLEAN UP DIRECTORY STARTED\n",
      "2020-09-10T09:56:13.532031 :: RENAME/CLEAN UP DIRECTORY COMPLETE\n",
      "\n",
      "\n",
      "2020-09-10T09:56:13.534034 :: DOWNLOAD START :: URL = https://earthobs.defra.gov.uk/geoserver/ows/?access_token=3FKZcj7oGza5DiZ7KBUCVbhMMYLDTl&SERVICE=WPS&VERSION=1.0.0&REQUEST=GetExecutionResult&EXECUTIONID=bf74982d-ae35-4e82-9695-a9a3f0629665&outputId=result.zip&mimetype=application/zip\n",
      "2020-09-10T09:57:05.099646 :: DOWNLOAD COMPLETE\n",
      "2020-09-10T09:57:05.100648 :: FILE WRITE START :: FILE = C:\\Users\\Kostas.Sideris\\output\\20200910T094613-Test1\\S2A_20190422_lat52lon07_T30UYC_ORB137_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.zip\n",
      "2020-09-10T09:58:11.304901 :: FILE WRITE COMPLETE\n",
      "2020-09-10T09:58:11.305900 :: EXTRACT ARCHIVE FILE STARTED\n",
      "2020-09-10T09:58:49.061240 :: EXTRACT ARCHIVE FILE COMPLETE\n",
      "2020-09-10T09:58:49.062238 :: RENAME/CLEAN UP DIRECTORY STARTED\n",
      "2020-09-10T09:58:49.427167 :: RENAME/CLEAN UP DIRECTORY COMPLETE\n",
      "\n",
      "\n",
      "2020-09-10T09:58:49.428167 :: #### SCRIPT FINISHED\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOAD, WRITE FILE AND RENAME FILE\n",
    "for index, row in filter_df.iterrows():\n",
    "    if row['wps:Status'] == 'SUCCEEDED':\n",
    "        print('\\n\\n' + datetime.utcnow().isoformat() + ' :: DOWNLOAD START :: URL = ' + row['dl_url'])        \n",
    "        response = requests.get(row['dl_url'],headers=headers);\n",
    "        print(datetime.utcnow().isoformat() + ' :: DOWNLOAD COMPLETE')\n",
    "        local_file_name = my_path / str(row['layer_name'].split(':')[-1] + '.zip')\n",
    "        print(datetime.utcnow().isoformat() + ' :: FILE WRITE START :: FILE = ' + str(local_file_name))        \n",
    "        with open(local_file_name, 'wb') as f:\n",
    "            f.write(response.content);\n",
    "            f.close();        \n",
    "        print(datetime.utcnow().isoformat() + ' :: FILE WRITE COMPLETE')\n",
    "        print(datetime.utcnow().isoformat() + ' :: EXTRACT ARCHIVE FILE STARTED')\n",
    "                \n",
    "        # TODO. Check if downloaded file is actually a zip file type. Handle exception for an xml error report?\n",
    "        zip_ref = ZipFile(local_file_name, 'r')\n",
    "        zip_ref.extractall(my_path)\n",
    "        zip_ref.close()\n",
    "        print(datetime.utcnow().isoformat() + ' :: EXTRACT ARCHIVE FILE COMPLETE')\n",
    "        print(datetime.utcnow().isoformat() + ' :: RENAME/CLEAN UP DIRECTORY STARTED')\n",
    "        for f in zip_ref.filelist:\n",
    "            if f.filename.endswith(\".sld\"):\n",
    "                Path(my_path / f.filename).unlink()\n",
    "            \n",
    "            # rename the uuid.tiff to layer_name.tif\n",
    "            elif f.filename.endswith(\".tif\") or f.filename.endswith(\".tiff\"):\n",
    "                Path(my_path / f.filename).rename(my_path / str(row['layer_name'].split(':')[-1] + '.tif'))\n",
    "            else:\n",
    "                raise ValueError(\"ERROR - some unknown file has been given:\", f)\n",
    "        \n",
    "        # remove the zip file from disk\n",
    "        Path(local_file_name).unlink()\n",
    "        print(datetime.utcnow().isoformat() + ' :: RENAME/CLEAN UP DIRECTORY COMPLETE')\n",
    "\n",
    "print('\\n\\n' + datetime.utcnow().isoformat() + ' :: #### SCRIPT FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries for Mosaic creation\n",
    "import rsgislib\n",
    "import rsgislib.imageutils\n",
    "import rsgislib.rastergis\n",
    "import rsgislib.imagecalc\n",
    "import rsgislib.imagecalc.calcindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the folder and the tif files for the mosaic\n",
    "dirpath= my_path\n",
    "search_critiria = \"*.tif\"\n",
    "q = os.path.join(dirpath,search_critiria)\n",
    "S2_fps = glob.glob(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing libraries for converting .tif to .kea for the Mosaic creation\n",
    "import os.path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the convertion \n",
    "def replaceGTIFF_kea(inputtext):\n",
    "    outputtext = \"\"    \n",
    "    for w in inputtext:\n",
    "        w = w.replace(\"GEOTIFF\",\"KEA\")\n",
    "        w = w.replace(\".TIF\",\".kea\")\n",
    "        # this line should be unnecessary since Landsat MTL files use capital letters       \n",
    "        # but just in case you have one that doesn't\n",
    "        w = w.replace(\".tif\",\".kea\")\n",
    "        outputtext += w\n",
    "    return outputtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all *.TIF files and *MTL.txt files in the current directory\n",
    "directory = os.chdir(my_path)\n",
    "dirFileList = os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dirFileList\n",
    "tifFileList = [f for f in dirFileList if ((f[-4:]=='.TIF')or(f[-4:]=='.tif'))]\n",
    "MTLFileList = [f for f in dirFileList if (f[-7:]=='MTL.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output format (GDAL code)\n",
    "outFormat = 'KEA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdal_translate -of KEA S2A_20190330_lat52lon075_T30UXC_ORB094_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif S2A_20190330_lat52lon075_T30UXC_ORB094_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.kea\n",
      "gdal_translate -of KEA S2A_20190330_lat52lon07_T30UYC_ORB094_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif S2A_20190330_lat52lon07_T30UYC_ORB094_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.kea\n",
      "gdal_translate -of KEA S2A_20190422_lat52lon07_T30UYC_ORB137_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif S2A_20190422_lat52lon07_T30UYC_ORB137_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.kea\n"
     ]
    }
   ],
   "source": [
    "# run gdal_translate on all TIFs to convert to KEA\n",
    "for t in tifFileList:\n",
    "    gdaltranscmd = \"gdal_translate -of \"+outFormat+\" \"+t+\" \"+t[:-4]+\".kea\"\n",
    "    print (gdaltranscmd)\n",
    "    os.system(gdaltranscmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new header file referring to .kea files rather than .TIF\n",
    "for m in MTLFileList:\n",
    "    inputtext = file(m).readlines()\n",
    "    outputtext = replaceGTIFF_kea(inputtext)\n",
    "    outputfilebase = m[:-4]\n",
    "    outputfile = outputfilebase + \"_kea.txt\"\n",
    "    out = file(outputfile,\"w\")\n",
    "    out.write(outputtext)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the paths for finding the created .kea files\n",
    "dirpath= my_path\n",
    "search_critiria_kea = \"*.kea\"\n",
    "q = os.path.join(dirpath,search_critiria_kea)\n",
    "S2 = glob.glob(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Kostas.Sideris\\\\output\\\\20200828T075639-Test\\\\S2A_20190330_lat52lon075_T30UXC_ORB094_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.kea',\n",
       " 'C:\\\\Users\\\\Kostas.Sideris\\\\output\\\\20200828T075639-Test\\\\S2A_20190330_lat52lon07_T30UYC_ORB094_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.kea',\n",
       " 'C:\\\\Users\\\\Kostas.Sideris\\\\output\\\\20200828T075639-Test\\\\S2A_20190422_lat52lon07_T30UYC_ORB137_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.kea']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mosaicing all the .kea files \n",
    "inImages = S2\n",
    "refImg = FG_Sentinel_ROI.kea\n",
    "outCompImg = CompSentinel.kea\n",
    "outMskImg = Mask.kea\n",
    "\n",
    "rsgislib.imageutils.imagecomp.createMaxNDVINDWIComposite(refImg, inImages, 3, 8, 9, outRefImg, outCompImg, outMskImg, tmpPath='./tmp', gdalformat='KEA', dataType=None, calcStats=True, reprojmethod='cubic', use_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the created .kea mosaic back to .tif \n",
    "def replaceGTIFF_kea(inputtext):\n",
    "    outputtext = \"\"    \n",
    "    for w in inputtext:\n",
    "        w = w.replace(\"KEA\",\"GEOTIFF\")\n",
    "        w = w.replace(\".kea\",\".TIF\")\n",
    "        # this line should be unnecessary since Landsat MTL files use capital letters       \n",
    "        # but just in case you have one that doesn't\n",
    "        w = w.replace(\".kea\",\".tif\")\n",
    "        outputtext += w\n",
    "    return outputtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all *.kea files and *MTL.txt files in the current directory\n",
    "directory = os.chdir(\"C:/Users/Kostas.Sideris/output/Mosaic\")\n",
    "dirFileList = os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dirFileList\n",
    "tifFileList = [f for f in dirFileList if ((f[-4:]=='.KEA')or(f[-4:]=='.kea'))]\n",
    "MTLFileList = [f for f in dirFileList if (f[-7:]=='MTL.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output format (GDAL code)\n",
    "outFormat = 'GTiff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdal_translate -of GTiff mosaic.kea mosaic.tif\n"
     ]
    }
   ],
   "source": [
    "#run gdal_translate on the .KEA Mosaic  to convert to .tiff\n",
    "for t in tifFileList:\n",
    "    gdaltranscmd = \"gdal_translate -of \"+outFormat+\" \"+t+\" \"+t[:-4]+\".tif\"\n",
    "    print (gdaltranscmd)\n",
    "    os.system(gdaltranscmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
